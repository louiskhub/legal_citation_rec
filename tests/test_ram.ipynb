{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louiskapp/miniconda3/envs/bva/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import random\n",
    "if not os.getcwd().endswith(\"legal_citation_rec\"):\n",
    "    os .chdir(\"..\")\n",
    "from utils import init_tokenizer\n",
    "from config import VOCAB_SIZES, ICLOUD_FP, TEXT_FP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NVIDIA Tesla T4 used by Google Colab has 16GB of RAM available and the CPU about 13GB.\n",
    "In the following cells, I will test RAM consumption for different dataset sizes and configurations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's find out the necessary datatype for our input tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token index: 30524\n"
     ]
    }
   ],
   "source": [
    "tokenizer, _, _ = init_tokenizer()\n",
    "print(f\"Max token index: {len(tokenizer)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum integer needed to represent the tokenizer indices is lower than `32,767`. This is great because it means that we can represent each index with a 16-bit integer. This will save us a substantial amount of memory compared to using int32."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce disk I/O. We could store the preprocessed input tokens in a single binary file instead of multiple ones. Let's find out if this is feasible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find out how many individual files exist per vocab size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1431: 317568, 859: 237831, 479: 266993, 105: 13413}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_fp: str = os.path.join(ICLOUD_FP, TEXT_FP)\n",
    "\n",
    "file_counts = {vsize: 0 for vsize in VOCAB_SIZES}\n",
    "for key in file_counts.keys():\n",
    "    file_counts[key] = len(os.listdir(os.path.join(preprocessed_fp, f\"vocab_size_{key}\")))\n",
    "    \n",
    "file_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate the average number of samples in a file from 500 random samples for each vocab size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of samples in a file per vocab: {1431: 2, 859: 2, 479: 2, 105: 2}.\n"
     ]
    }
   ],
   "source": [
    "n = 500\n",
    "nsamples = {vsize: 0 for vsize in VOCAB_SIZES}\n",
    "for vsize in VOCAB_SIZES:\n",
    "    dir_fp: str = os.path.join(preprocessed_fp, f\"vocab_size_{vsize}\")\n",
    "    fnames = os.listdir(dir_fp)\n",
    "    random.shuffle(fnames)\n",
    "    for f in fnames[:n]:\n",
    "        t = torch.load(os.path.join(dir_fp, f))\n",
    "        nsamples[vsize] += len(t)\n",
    "    nsamples[vsize] //= 500\n",
    "    \n",
    "print(f\"Average number of samples in a file per vocab: {nsamples}.\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's allocate a tensor that includes all data samples for each vocab size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needed RAM in GB: {1431: 2.6116800000000002, 859: 1.9559229120000001, 479: 2.1957512, 105: 0.11030928000000001}\n"
     ]
    }
   ],
   "source": [
    "ram_size_in_gb = dict()\n",
    "for vsize in VOCAB_SIZES:\n",
    "    t1 = torch.randint( # inputs\n",
    "        low=0,\n",
    "        high=30524,\n",
    "        size=(\n",
    "            nsamples[vsize] * file_counts[vsize],\n",
    "            256,\n",
    "        ),\n",
    "        dtype=torch.int16,\n",
    "    )\n",
    "    t2 = torch.randint( # labels\n",
    "        low=0,\n",
    "        high=30524,\n",
    "        size=(\n",
    "            nsamples[vsize] * file_counts[vsize],\n",
    "            1,\n",
    "        ),\n",
    "        dtype=torch.int16,\n",
    "    )\n",
    "    size_in_bit: int = sys.getsizeof(t1.storage()) + sys.getsizeof(t2.storage())\n",
    "    ram_size_in_gb[vsize] = size_in_bit * 8e-9\n",
    "    \n",
    "print(f\"Needed RAM in GB: {ram_size_in_gb}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the whole dataset into memory at once seems feasible for every vocabulary size!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after I united the data into bigger chunks, it gets obvious that my approximations were flawed.\n",
    "For processing speed optimation, I did not create a single large .pt file per vocab size (yet).\n",
    "Instead, I created files of 20000 samples each. These will be concatenated to a single large .pt file.\n",
    "\n",
    "Let's see how many files were created for each vocab size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of context files (= n label files) for vocab size 1431: 74.0\n",
      "Final united tensor length for vocab size 1431: 1480000\n",
      "Number of context files (= n label files) for vocab size 859: 40.0\n",
      "Final united tensor length for vocab size 859: 800000\n",
      "Number of context files (= n label files) for vocab size 479: 27.0\n",
      "Final united tensor length for vocab size 479: 540000\n",
      "Number of context files (= n label files) for vocab size 105: 8.0\n",
      "Final united tensor length for vocab size 105: 160000\n"
     ]
    }
   ],
   "source": [
    "all_file_names: list[str] = os.listdir(os.path.join(\"data\",\"text\",\"preprocessed\"))\n",
    "n: int = 20000\n",
    "tensor_lengths = dict()\n",
    "\n",
    "for vsize in VOCAB_SIZES:\n",
    "    file_names_per_vocab: list[str] = [fname for fname in all_file_names if fname.endswith(f\"{vsize}.pt\")]\n",
    "    print(f\"Number of context files (= n label files) for vocab size {vsize}: {len(file_names_per_vocab)/2}\")\n",
    "    tensor_lengths[vsize] = (len(file_names_per_vocab)//2)*n\n",
    "    print(f\"Final united tensor length for vocab size {vsize}: {tensor_lengths[vsize]}\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's allocate a tensor that includes all data samples for each vocab size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needed RAM in GB: {1431: 6.085760768, 859: 3.289600768, 479: 2.2204807680000003, 105: 0.657920768}\n"
     ]
    }
   ],
   "source": [
    "ram_size_in_gb = dict()\n",
    "for vsize in VOCAB_SIZES:\n",
    "    t1 = torch.randint( # inputs\n",
    "        low=0,\n",
    "        high=30524,\n",
    "        size=(\n",
    "            tensor_lengths[vsize],\n",
    "            256,\n",
    "        ),\n",
    "        dtype=torch.int16,\n",
    "    )\n",
    "    t2 = torch.randint( # labels\n",
    "        low=0,\n",
    "        high=30524,\n",
    "        size=(\n",
    "            tensor_lengths[vsize],\n",
    "            1,\n",
    "        ),\n",
    "        dtype=torch.int16,\n",
    "    )\n",
    "    size_in_bit: int = sys.getsizeof(t1.storage()) + sys.getsizeof(t2.storage())\n",
    "    ram_size_in_gb[vsize] = size_in_bit * 8e-9\n",
    "    \n",
    "print(f\"Needed RAM in GB: {ram_size_in_gb}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This changes things quite a bit. While 105 and 479 vocab size seems still feasible, the others are probably not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
